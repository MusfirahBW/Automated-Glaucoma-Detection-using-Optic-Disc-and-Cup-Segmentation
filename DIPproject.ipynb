{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531d76a7",
   "metadata": {},
   "source": [
    "# Digital Image Processing Project Section A\n",
    "## Glaucoma Detection\n",
    "##### Memebrs:\n",
    "- Syed Qasim Hussain 21i-0379\n",
    "- Hashir Saeed 21i-0376\n",
    "- Musfirah 21i-0789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a89ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, jaccard_score\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936bab0",
   "metadata": {},
   "source": [
    "# find_optic_disc(image_path):\n",
    "### This function takes the path of an image as input and attempts to find the optic disc region within the image. Here's how it works:\n",
    "- It reads the image using cv2.imread() and converts it to grayscale using cv2.cvtColor().\n",
    "- It applies Gaussian Blur to the grayscale image using cv2.GaussianBlur() to reduce noise.\n",
    "- It detects edges in the blurred image using the Canny edge detector (cv2.Canny()).\n",
    "- It finds contours in the edge image using cv2.findContours().\n",
    "- It loops through each contour and checks if its area is greater than 150 pixels.\n",
    "- For contours with a large enough area, it calculates the mean intensity within the contour region.\n",
    "- If the mean intensity is above a certain brightness threshold (120 for TIFF images, 1 for others), it considers the contour as a potential optic disc candidate.\n",
    "- If any optic disc candidates are found, it selects the largest contour and returns its bounding box coordinates (x, y, x+w, y+h).\n",
    "- If no optic disc candidates are found, it returns None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39d9c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find optic disc\n",
    "def find_optic_disc(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edges = cv2.Canny(blurred, 100, 200)\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    disc_contours = []\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > 150:\n",
    "            mask = np.zeros_like(gray)\n",
    "            cv2.drawContours(mask, [contour], 0, 255, -1)\n",
    "            mean_intensity = cv2.mean(img, mask=mask)[0]\n",
    "\n",
    "            if image_path.endswith(\".tif\"):\n",
    "                brightness_threshold = 120\n",
    "            else:\n",
    "                brightness_threshold = 1\n",
    "\n",
    "            if mean_intensity > brightness_threshold:\n",
    "                disc_contours.append(contour)\n",
    "\n",
    "    if disc_contours:\n",
    "        largest_contour = max(disc_contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        return x, y, x+w, y+h\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36702757",
   "metadata": {},
   "source": [
    "# auto_annotate(data_path):\n",
    "### This function automatically annotates the optic disc regions in all images (PNG and TIFF) within the specified data_path. It does this by:\n",
    "- Iterating over all image files in the data_path directory.\n",
    "- For each image, it calls the find_optic_disc() function to get the optic disc bounding box coordinates.\n",
    "- If a bounding box is found, it appends a tuple containing the image name and bounding box coordinates to the annotations list.\n",
    "- After processing all images, it returns the annotations list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2dbf188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to auto annotate\n",
    "def auto_annotate(data_path):\n",
    "    annotations = []\n",
    "    for image_name in os.listdir(data_path):\n",
    "        if image_name.endswith((\".png\", \".tif\")):\n",
    "            image_path = os.path.join(data_path, image_name)\n",
    "            bbox = find_optic_disc(image_path)\n",
    "            if bbox is not None:\n",
    "                annotations.append((image_name, *bbox))\n",
    "    return annotations\n",
    "\n",
    "# Function to save annotations to CSV\n",
    "def save_annotations_to_csv(annotations, output_file):\n",
    "    with open(output_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Filename', 'Xmin', 'Ymin', 'Xmax', 'Ymax'])\n",
    "        for annotation in annotations:\n",
    "            writer.writerow(annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9297ac",
   "metadata": {},
   "source": [
    "### This function creates binary masks for the optic disc regions based on the annotations. It does this by:\n",
    "- Iterating over each annotation (image name and bounding box coordinates).\n",
    "- Loading the corresponding image using cv2.imread().\n",
    "- Creating a blank mask (numpy array of zeros) with the same size as the original image.\n",
    "- Drawing a filled rectangle on the mask within the bounding box coordinates.\n",
    "- Resizing the mask to the specified img_size.Appending the resized mask to the masks list.Finally, it returns a numpy array of all masks, with values scaled between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c182728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(data_path, annotations, img_size=(256, 256)):\n",
    "    masks = []\n",
    "    for annotation in annotations:\n",
    "        image_name, xmin, ymin, xmax, ymax = annotation\n",
    "        img_path = os.path.join(data_path, image_name)\n",
    "        mask = np.zeros((1376, 1376), dtype=np.uint8)\n",
    "        xmin, ymin, xmax, ymax = map(int, [xmin, ymin, xmax, ymax])\n",
    "        mask[ymin:ymax, xmin:xmax] = 255\n",
    "        mask_resized = cv2.resize(mask, img_size)\n",
    "        masks.append(mask_resized)\n",
    "    return np.array(masks) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152fc2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_labels(data_path, annotations, img_size=(256, 256)):\n",
    "    images = []\n",
    "    masks = create_masks(data_path, annotations, img_size)\n",
    "    for annotation in annotations:\n",
    "        image_name, _, _, _, _ = annotation\n",
    "        img_path = os.path.join(data_path, image_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        img_resized = cv2.resize(img, img_size)\n",
    "        images.append(img_resized)\n",
    "    images = np.array(images) / 255.0\n",
    "    masks = np.expand_dims(masks, axis=-1)\n",
    "    return images, masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded62767",
   "metadata": {},
   "source": [
    "### This function loads the images and their corresponding masks (labels) based on the annotations. It does this by:\n",
    "- Calling create_masks() to generate the masks.\n",
    "- Iterating over each annotation (image name and bounding box coordinates).\n",
    "- Loading the corresponding image using cv2.imread().\n",
    "- Resizing the image to the specified img_size.\n",
    "- Appending the resized image to the images list.\n",
    "- Converting the images list to a numpy array and scaling the pixel values between 0 and 1.\n",
    "- Adding an extra channel dimension to the masks array.\n",
    "- Returning the images and masks arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc1cefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = 'C:/Users/DELL/Desktop/RIGA 200 Images'\n",
    "\n",
    "# Load annotations\n",
    "annotations = auto_annotate(images_dir)\n",
    "save_annotations_to_csv(annotations, 'annotations.csv')\n",
    "\n",
    "# Shuffle and split dataset\n",
    "annotations_df = pd.read_csv('annotations.csv').sample(frac=1, random_state=42)\n",
    "train_annotations, test_annotations = train_test_split(annotations_df.values, test_size=0.2, random_state=42)\n",
    "train_annotations, val_annotations = train_test_split(train_annotations, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load images and labels\n",
    "train_images, train_masks = load_images_and_labels(images_dir, train_annotations)\n",
    "val_images, val_masks = load_images_and_labels(images_dir, val_annotations)\n",
    "test_images, test_masks = load_images_and_labels(images_dir, test_annotations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1fb3ba",
   "metadata": {},
   "source": [
    "### UNET Model\n",
    "- This function defines the architecture of the U-Net model for segmentation tasks. It uses convolutional layers, max-pooling layers, and transposed convolutional layers (for upsampling) to create the encoder and decoder parts of the U-Net. The model takes an input image of size input_size and outputs a binary mask of the same spatial dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "641110c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(input_size=(256, 256, 3)):\n",
    "    inputs = layers.Input(input_size)\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Dropout(0.1)(c1)\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Dropout(0.1)(c2)\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Dropout(0.2)(c3)\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = layers.Dropout(0.2)(c4)\n",
    "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "    c5 = layers.Dropout(0.3)(c5)\n",
    "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    u6 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = layers.Dropout(0.2)(c6)\n",
    "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = layers.Dropout(0.2)(c7)\n",
    "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    c8 = layers.Dropout(0.1)(c8)\n",
    "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = layers.concatenate([u9, c1])\n",
    "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    c9 = layers.Dropout(0.1)(c9)\n",
    "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd4108",
   "metadata": {},
   "source": [
    "#### train_model(model, train_images, train_labels, val_images, val_labels):\n",
    "- This function trains the provided model using the training data (train_images and train_labels) and validation data (val_images and val_labels). It does this by:\n",
    "- Compiling the model with the Adam optimizer and binary cross-entropy loss.\n",
    "- Creating data generators for the training and validation data using ImageDataGenerator().\n",
    "- Fitting the model to the training data generator for 50 epochs, using the validation data generator for validation.\n",
    "- Returning the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba7de2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model function\n",
    "def train_model(model, train_images, train_labels, val_images, val_labels):\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    datagen = ImageDataGenerator()\n",
    "    train_gen = datagen.flow(train_images, train_labels, shuffle=True)\n",
    "    val_gen = datagen.flow(val_images, val_labels, shuffle=True)\n",
    "\n",
    "    history = model.fit(train_gen, epochs=50, validation_data=val_gen)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2053913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Desktop\\proj_1\\env\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 41s/step - accuracy: 0.6589 - loss: 0.6918 - val_accuracy: 0.7005 - val_loss: 0.6687\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 39s/step - accuracy: 0.7009 - loss: 0.6175 - val_accuracy: 0.6988 - val_loss: 1.8267\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 35s/step - accuracy: 0.7585 - loss: 1.3112 - val_accuracy: 0.7693 - val_loss: 0.6278\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 36s/step - accuracy: 0.7201 - loss: 0.6440 - val_accuracy: 0.7695 - val_loss: 0.6304\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7640 - loss: 0.6289 - val_accuracy: 0.7728 - val_loss: 0.6112\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7279 - loss: 0.6128 - val_accuracy: 0.7443 - val_loss: 0.5322\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 35s/step - accuracy: 0.7278 - loss: 0.5183 - val_accuracy: 0.7687 - val_loss: 0.4507\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 36s/step - accuracy: 0.7578 - loss: 0.4661 - val_accuracy: 0.7769 - val_loss: 0.4358\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 36s/step - accuracy: 0.7497 - loss: 0.4514 - val_accuracy: 0.7900 - val_loss: 0.5145\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 38s/step - accuracy: 0.7520 - loss: 0.5508 - val_accuracy: 0.7694 - val_loss: 0.5644\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 37s/step - accuracy: 0.7441 - loss: 0.5786 - val_accuracy: 0.7306 - val_loss: 0.5431\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7315 - loss: 0.5449 - val_accuracy: 0.7781 - val_loss: 0.5003\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7292 - loss: 0.5260 - val_accuracy: 0.7849 - val_loss: 0.4639\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7404 - loss: 0.4773 - val_accuracy: 0.7889 - val_loss: 0.4475\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7458 - loss: 0.4631 - val_accuracy: 0.7914 - val_loss: 0.4491\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 37s/step - accuracy: 0.7390 - loss: 0.4876 - val_accuracy: 0.7725 - val_loss: 0.4456\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 35s/step - accuracy: 0.7681 - loss: 0.4375 - val_accuracy: 0.7872 - val_loss: 0.4425\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7660 - loss: 0.4459 - val_accuracy: 0.7946 - val_loss: 0.4378\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7776 - loss: 0.4322 - val_accuracy: 0.7929 - val_loss: 0.4296\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7578 - loss: 0.4322 - val_accuracy: 0.7904 - val_loss: 0.4104\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 36s/step - accuracy: 0.7391 - loss: 0.4331 - val_accuracy: 0.7713 - val_loss: 0.3905\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7451 - loss: 0.4201 - val_accuracy: 0.7750 - val_loss: 0.3592\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7361 - loss: 0.4022 - val_accuracy: 0.8041 - val_loss: 0.3469\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7549 - loss: 0.3787 - val_accuracy: 0.8018 - val_loss: 0.3459\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 35s/step - accuracy: 0.7568 - loss: 0.3736 - val_accuracy: 0.8110 - val_loss: 0.3475\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 35s/step - accuracy: 0.7603 - loss: 0.3794 - val_accuracy: 0.8101 - val_loss: 0.3436\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 36s/step - accuracy: 0.7702 - loss: 0.3952 - val_accuracy: 0.7969 - val_loss: 0.5382\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 36s/step - accuracy: 0.7460 - loss: 0.4673 - val_accuracy: 0.7870 - val_loss: 0.3467\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 35s/step - accuracy: 0.7467 - loss: 0.4059 - val_accuracy: 0.7890 - val_loss: 0.3462\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 35s/step - accuracy: 0.7422 - loss: 0.3632 - val_accuracy: 0.7733 - val_loss: 0.3575\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7609 - loss: 0.3802 - val_accuracy: 0.7731 - val_loss: 0.3577\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 36s/step - accuracy: 0.7415 - loss: 0.3853 - val_accuracy: 0.7841 - val_loss: 0.3490\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 35s/step - accuracy: 0.7484 - loss: 0.3647 - val_accuracy: 0.7891 - val_loss: 0.3520\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7448 - loss: 0.4028 - val_accuracy: 0.7894 - val_loss: 0.3469\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7415 - loss: 0.3854 - val_accuracy: 0.7931 - val_loss: 0.3477\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7633 - loss: 0.3748 - val_accuracy: 0.7955 - val_loss: 0.3471\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7509 - loss: 0.3587 - val_accuracy: 0.8014 - val_loss: 0.3471\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 36s/step - accuracy: 0.7658 - loss: 0.3586 - val_accuracy: 0.7984 - val_loss: 0.3460\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 35s/step - accuracy: 0.7376 - loss: 0.4114 - val_accuracy: 0.7983 - val_loss: 0.3444\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7569 - loss: 0.3737 - val_accuracy: 0.8072 - val_loss: 0.3443\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 36s/step - accuracy: 0.7655 - loss: 0.3767 - val_accuracy: 0.8040 - val_loss: 0.3439\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 35s/step - accuracy: 0.7846 - loss: 0.3331 - val_accuracy: 0.8079 - val_loss: 0.3412\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 35s/step - accuracy: 0.7584 - loss: 0.3798 - val_accuracy: 0.8087 - val_loss: 0.3394\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7688 - loss: 0.3539 - val_accuracy: 0.8098 - val_loss: 0.3385\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7739 - loss: 0.3612 - val_accuracy: 0.8101 - val_loss: 0.3380\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 35s/step - accuracy: 0.7748 - loss: 0.3456 - val_accuracy: 0.8065 - val_loss: 0.3403\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 36s/step - accuracy: 0.7870 - loss: 0.3446 - val_accuracy: 0.8006 - val_loss: 0.3377\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7663 - loss: 0.3687 - val_accuracy: 0.8057 - val_loss: 0.3374\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 35s/step - accuracy: 0.7637 - loss: 0.3545 - val_accuracy: 0.8070 - val_loss: 0.3405\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7546 - loss: 0.3777 - val_accuracy: 0.7972 - val_loss: 0.3404\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train U-Net model for optic disc segmentation\n",
    "od_model = unet_model(input_size=(256, 256, 3))\n",
    "history_od = train_model(od_model, train_images, train_masks, val_images, val_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7cf5aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2s/step - accuracy: 0.8227 - loss: 0.2762\n",
      "Test Loss: 0.2747751772403717, Test Accuracy: 0.8222770690917969\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_images, test_labels):\n",
    "    results = model.evaluate(test_images, test_labels)\n",
    "    print(f\"Test Loss: {results[0]}, Test Accuracy: {results[1]}\")\n",
    "\n",
    "evaluate_model(od_model, test_images, test_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f82d0",
   "metadata": {},
   "source": [
    "#### refine_oc_segmentation(od_image, oc_model):\n",
    "- This function takes an optic disc image (od_image) and the optic cup model (oc_model) as input. It refines the optic cup segmentation by:\n",
    "- Predicting the optic cup mask using the optic cup model (oc_model.predict()).\n",
    "- Returning the predicted optic cup mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eaa1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optic cup segmentation refinement function\n",
    "def refine_oc_segmentation(od_image, oc_model):\n",
    "    oc_mask = oc_model.predict(np.expand_dims(od_image, axis=0))[0]\n",
    "    return oc_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a58da4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Load images and labels for optic cup\n",
    "train_images_oc, train_masks_oc = load_images_and_labels(images_dir, train_annotations)\n",
    "test_images_oc, test_masks_oc = load_images_and_labels(images_dir, test_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "283c828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 39s/step - accuracy: 0.6701 - loss: 0.6911 - val_accuracy: 0.7694 - val_loss: 0.6707\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 38s/step - accuracy: 0.7549 - loss: 0.6575 - val_accuracy: 0.7696 - val_loss: 0.5771\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 38s/step - accuracy: 0.7218 - loss: 0.6195 - val_accuracy: 0.7694 - val_loss: 0.5663\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 37s/step - accuracy: 0.7038 - loss: 0.8309 - val_accuracy: 0.7696 - val_loss: 0.5557\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7485 - loss: 0.5858 - val_accuracy: 0.7760 - val_loss: 0.6027\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 38s/step - accuracy: 0.7536 - loss: 0.6088 - val_accuracy: 0.7524 - val_loss: 0.5623\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 38s/step - accuracy: 0.7097 - loss: 0.8033 - val_accuracy: 0.7862 - val_loss: 0.5937\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 39s/step - accuracy: 0.7459 - loss: 0.6154 - val_accuracy: 0.7709 - val_loss: 0.6178\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 41s/step - accuracy: 0.7472 - loss: 0.6282 - val_accuracy: 0.7675 - val_loss: 0.6187\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 39s/step - accuracy: 0.7209 - loss: 0.6366 - val_accuracy: 0.7727 - val_loss: 0.6141\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 36s/step - accuracy: 0.7476 - loss: 0.6256 - val_accuracy: 0.7841 - val_loss: 0.6057\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 36s/step - accuracy: 0.7414 - loss: 0.6150 - val_accuracy: 0.7894 - val_loss: 0.5924\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 36s/step - accuracy: 0.7344 - loss: 0.6013 - val_accuracy: 0.7792 - val_loss: 0.5741\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 36s/step - accuracy: 0.7506 - loss: 0.5772 - val_accuracy: 0.7942 - val_loss: 0.5445\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7421 - loss: 0.5581 - val_accuracy: 0.7845 - val_loss: 0.5092\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 40s/step - accuracy: 0.7474 - loss: 0.5153 - val_accuracy: 0.7885 - val_loss: 0.4637\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 37s/step - accuracy: 0.7366 - loss: 0.4808 - val_accuracy: 0.7897 - val_loss: 0.4426\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 36s/step - accuracy: 0.7573 - loss: 0.4546 - val_accuracy: 0.7886 - val_loss: 0.4255\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7579 - loss: 0.4430 - val_accuracy: 0.7833 - val_loss: 0.4051\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 43s/step - accuracy: 0.7687 - loss: 0.3951 - val_accuracy: 0.7892 - val_loss: 0.4354\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 40s/step - accuracy: 0.7543 - loss: 0.4363 - val_accuracy: 0.7472 - val_loss: 0.3565\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 43s/step - accuracy: 0.7273 - loss: 0.3884 - val_accuracy: 0.7554 - val_loss: 0.3668\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 37s/step - accuracy: 0.7698 - loss: 0.3415 - val_accuracy: 0.7637 - val_loss: 0.3629\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 35s/step - accuracy: 0.7385 - loss: 0.3968 - val_accuracy: 0.7680 - val_loss: 0.3594\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 36s/step - accuracy: 0.7231 - loss: 0.3949 - val_accuracy: 0.7695 - val_loss: 0.3619\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 36s/step - accuracy: 0.7524 - loss: 0.3669 - val_accuracy: 0.7912 - val_loss: 0.3492\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7688 - loss: 0.3559 - val_accuracy: 0.7899 - val_loss: 0.3475\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7633 - loss: 0.3812 - val_accuracy: 0.8004 - val_loss: 0.3475\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 35s/step - accuracy: 0.7359 - loss: 0.3823 - val_accuracy: 0.7938 - val_loss: 0.3477\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7580 - loss: 0.3614 - val_accuracy: 0.7977 - val_loss: 0.3445\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 35s/step - accuracy: 0.7440 - loss: 0.3839 - val_accuracy: 0.7941 - val_loss: 0.3438\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 36s/step - accuracy: 0.7680 - loss: 0.3602 - val_accuracy: 0.8035 - val_loss: 0.3433\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7595 - loss: 0.3624 - val_accuracy: 0.8055 - val_loss: 0.3440\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7699 - loss: 0.3732 - val_accuracy: 0.8095 - val_loss: 0.3431\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 35s/step - accuracy: 0.7724 - loss: 0.3608 - val_accuracy: 0.8084 - val_loss: 0.3426\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7770 - loss: 0.3679 - val_accuracy: 0.7957 - val_loss: 0.3490\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7664 - loss: 0.3730 - val_accuracy: 0.7738 - val_loss: 0.4235\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 35s/step - accuracy: 0.7746 - loss: 0.3869 - val_accuracy: 0.7896 - val_loss: 0.3426\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7315 - loss: 0.3877 - val_accuracy: 0.7922 - val_loss: 0.3453\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 35s/step - accuracy: 0.7643 - loss: 0.3677 - val_accuracy: 0.8032 - val_loss: 0.3447\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7471 - loss: 0.3942 - val_accuracy: 0.8011 - val_loss: 0.3440\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7624 - loss: 0.3784 - val_accuracy: 0.8042 - val_loss: 0.3411\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 36s/step - accuracy: 0.7727 - loss: 0.3449 - val_accuracy: 0.8043 - val_loss: 0.3435\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7930 - loss: 0.3221 - val_accuracy: 0.8074 - val_loss: 0.3415\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 35s/step - accuracy: 0.7569 - loss: 0.3777 - val_accuracy: 0.8088 - val_loss: 0.3404\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7905 - loss: 0.3388 - val_accuracy: 0.7690 - val_loss: 0.3660\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 35s/step - accuracy: 0.7391 - loss: 0.3988 - val_accuracy: 0.7922 - val_loss: 0.3431\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 36s/step - accuracy: 0.7578 - loss: 0.3937 - val_accuracy: 0.7859 - val_loss: 0.3433\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 36s/step - accuracy: 0.7533 - loss: 0.3639 - val_accuracy: 0.7909 - val_loss: 0.3439\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 35s/step - accuracy: 0.7852 - loss: 0.3386 - val_accuracy: 0.7994 - val_loss: 0.3437\n"
     ]
    }
   ],
   "source": [
    "# Define U-Net model for Optic Cup\n",
    "oc_model = unet_model(input_size=(256, 256, 3))\n",
    "\n",
    "# Train OC model\n",
    "history_oc = train_model(oc_model, train_images_oc, train_masks_oc, val_images, val_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "969d2628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2s/step - accuracy: 0.8279 - loss: 0.2780\n",
      "Test Loss: 0.2767447829246521, Test Accuracy: 0.827144980430603\n"
     ]
    }
   ],
   "source": [
    "# Evaluate OC model\n",
    "evaluate_model(oc_model, test_images_oc, test_masks_oc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7dc5460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained OD model in the native Keras format\n",
    "od_model.save('od_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "04ea36b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained OC model in the native Keras format\n",
    "oc_model.save('oc_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3c9ff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Desktop\\proj_1\\env\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 48 variables whereas the saved optimizer has 94 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# # Load the saved OD model\n",
    "# od_model = load_model('od_model.keras', compile=False)\n",
    "\n",
    "# # Re-initialize the optimizer\n",
    "# od_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Load the saved OD model\n",
    "od_model = load_model('od_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54f59217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved OC model\n",
    "oc_model = load_model('oc_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45928f2d",
   "metadata": {},
   "source": [
    "### integrate_cdr_calculation(image_path, od_model, oc_model):\n",
    "#### This function integrates the optic disc and optic cup segmentation models to calculate the CDR for a given image. It does this by:\n",
    "- Loading the image using cv2.imread() and preprocessing it (resizing and scaling).\n",
    "- Predicting the optic disc mask using the optic disc model (od_model.predict()).\n",
    "- Refining the optic cup segmentation using the refine_oc_segmentation() function and the optic cup model (oc_model).\n",
    "- Calculating the CDR using the calculate_cdr() function with the optic disc and optic cup masks.\n",
    "- Returning the calculated CDR value.\n",
    "\n",
    "### calculate_cdr(od_mask, oc_mask):\n",
    "#### This function calculates the Cup-to-Disc Ratio (CDR) given the optic disc mask (od_mask) and optic cup mask (oc_mask). It does this by:\n",
    "- Counting the number of pixels above a threshold (0.5) in the optic disc mask to get the optic disc area (od_area).\n",
    "- Counting the number of pixels above a threshold (0.5) in the optic cup mask to get the optic cup area (oc_area).\n",
    "- Calculating the CDR by dividing the optic cup area by the optic disc area (oc_area / od_area).\n",
    "- If the optic disc area is 0, it returns 0 as the CDR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d99e236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CDR\n",
    "def calculate_cdr(od_mask, oc_mask):\n",
    "    od_area = np.sum(od_mask > 0.5)\n",
    "    oc_area = np.sum(oc_mask > 0.5)\n",
    "    cdr = oc_area / od_area if od_area > 0 else 0\n",
    "    return cdr\n",
    "\n",
    "def integrate_cdr_calculation(image_path, od_model, oc_model):\n",
    "    img = cv2.imread(image_path)\n",
    "    img_preprocessed = cv2.resize(img, (256, 256)) / 255.0\n",
    "\n",
    "    od_mask = od_model.predict(np.expand_dims(img_preprocessed, axis=0))[0]\n",
    "    oc_mask = refine_oc_segmentation(img_preprocessed, oc_model)\n",
    "\n",
    "    cdr = calculate_cdr(od_mask, oc_mask)\n",
    "    return cdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67dc0638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 809ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 765ms/step\n",
      "Cup-to-Disc Ratio (CDR) for sample image: 0.697761388739107\n"
     ]
    }
   ],
   "source": [
    "# Example usage for a sample image\n",
    "sample_image_path = 'C:/Users/DELL/Desktop/RIGA 200 Images/49.png'\n",
    "cdr_value = integrate_cdr_calculation(sample_image_path, od_model, oc_model)\n",
    "print(f\"Cup-to-Disc Ratio (CDR) for sample image: {cdr_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f0828",
   "metadata": {},
   "source": [
    "### evaluate_segmentation_performance(y_true, y_pred):\n",
    "### This function evaluates the segmentation performance of a model by calculating various metrics (accuracy, precision, recall, and IoU) given the true labels (y_true) and predicted masks (y_pred). It does this by:\n",
    "- Flattening the true labels and predicted masks, and converting them to binary arrays using a threshold of 0.5.\n",
    "- Calculating accuracy using accuracy_score() from sklearn.metrics.\n",
    "- Calculating precision using precision_score() from sklearn.metrics.\n",
    "- Calculating recall using recall_score() from sklearn.metrics.\n",
    "- Calculating IoU (Intersection over Union) using jaccard_score() from sklearn.metrics.\n",
    "- Returning the calculated metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfad58a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_segmentation_performance(y_true, y_pred):\n",
    "    y_true_flat = (y_true.flatten() > 0.5).astype(int)\n",
    "    y_pred_flat = (y_pred.flatten() > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_true_flat, y_pred_flat)\n",
    "    precision = precision_score(y_true_flat, y_pred_flat)\n",
    "    recall = recall_score(y_true_flat, y_pred_flat)\n",
    "    iou = jaccard_score(y_true_flat, y_pred_flat)\n",
    "    return accuracy, precision, recall, iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f1a68b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2s/step\n",
      "OD Segmentation - Accuracy: 0.8224220275878906, Precision: 0.5672413741795266, Recall: 0.9513494808537257, IoU: 0.5512507832457705\n"
     ]
    }
   ],
   "source": [
    "# Evaluate OD model performance\n",
    "od_accuracy, od_precision, od_recall, od_iou = evaluate_segmentation_performance(test_masks, od_model.predict(test_images))\n",
    "print(f\"OD Segmentation - Accuracy: {od_accuracy}, Precision: {od_precision}, Recall: {od_recall}, IoU: {od_iou}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c619ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2s/step\n",
      "OC Segmentation - Accuracy: 0.8272994995117188, Precision: 0.5773617766838919, Recall: 0.9210242179532611, IoU: 0.5501263991350801\n"
     ]
    }
   ],
   "source": [
    "# Evaluate OC model performance\n",
    "oc_accuracy, oc_precision, oc_recall, oc_iou = evaluate_segmentation_performance(test_masks_oc, oc_model.predict(test_images_oc))\n",
    "print(f\"OC Segmentation - Accuracy: {oc_accuracy}, Precision: {oc_precision}, Recall: {oc_recall}, IoU: {oc_iou}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29515364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
